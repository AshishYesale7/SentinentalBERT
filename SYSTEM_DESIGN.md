# SentinelBERT - Multi-Platform Sentiment & Behavioral Pattern Analysis System

## Executive Summary

SentinelBERT is a comprehensive social media intelligence platform designed for law enforcement and security agencies to monitor, analyze, and understand sentiment patterns and behavioral trends across multiple social media platforms. The system provides real-time ingestion, advanced NLP analysis using BERT, and intuitive visualization dashboards for investigative purposes.

---

## 1. High-Level System Architecture

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              SENTINELBERT SYSTEM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Data Sources  â”‚    â”‚  Ingestion      â”‚    â”‚   Processing    â”‚             â”‚
â”‚  â”‚                 â”‚    â”‚   Pipeline      â”‚    â”‚    Pipeline     â”‚             â”‚
â”‚  â”‚ â€¢ X.com API     â”‚â”€â”€â”€â–¶â”‚                 â”‚â”€â”€â”€â–¶â”‚                 â”‚             â”‚
â”‚  â”‚ â€¢ Instagram API â”‚    â”‚ â€¢ Rust ETL      â”‚    â”‚ â€¢ Python BERT   â”‚             â”‚
â”‚  â”‚ â€¢ Reddit API    â”‚    â”‚ â€¢ Rate Limiting â”‚    â”‚ â€¢ Sentiment     â”‚             â”‚
â”‚  â”‚ â€¢ Other APIs    â”‚    â”‚ â€¢ Data Cleaning â”‚    â”‚ â€¢ Behavioral    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                   â”‚                       â”‚                     â”‚
â”‚                                   â–¼                       â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚    Storage      â”‚             â”‚
â”‚  â”‚   Dashboard     â”‚    â”‚   Services      â”‚    â”‚    Layer        â”‚             â”‚
â”‚  â”‚                 â”‚â—€â”€â”€â”€â”‚                 â”‚â”€â”€â”€â–¶â”‚                 â”‚             â”‚
â”‚  â”‚ â€¢ React/Flutter â”‚    â”‚ â€¢ Spring Boot   â”‚    â”‚ â€¢ PostgreSQL    â”‚             â”‚
â”‚  â”‚ â€¢ Timeline View â”‚    â”‚ â€¢ REST APIs     â”‚    â”‚ â€¢ ElasticSearch â”‚             â”‚
â”‚  â”‚ â€¢ Analytics     â”‚    â”‚ â€¢ Orchestration â”‚    â”‚ â€¢ Redis Cache   â”‚             â”‚
â”‚  â”‚ â€¢ Region Filter â”‚    â”‚ â€¢ Auth Service  â”‚    â”‚ â€¢ File Storage  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Architecture

```
Data Sources â†’ Rust Ingestion â†’ Message Queue â†’ Python NLP â†’ Storage â†’ Spring Boot API â†’ Frontend
     â”‚              â”‚               â”‚              â”‚           â”‚            â”‚            â”‚
     â”‚              â”‚               â”‚              â”‚           â”‚            â”‚            â”‚
   APIs         Rate Limiting    Apache Kafka    BERT Model  PostgreSQL   REST APIs   Dashboard
 Scrapers       Data Cleaning    Redis Queue     Sentiment   ElasticSearch GraphQL    Timeline
 Webhooks       Validation       Pub/Sub         Behavioral  File Store   WebSocket   Analytics
```

### Core Components

1. **Data Ingestion Layer (Rust)**
   - High-performance concurrent data collection
   - API rate limiting and retry mechanisms
   - Real-time data streaming and buffering
   - Data validation and normalization

2. **Processing Layer (Python + BERT)**
   - Sentiment analysis using fine-tuned BERT models
   - Behavioral pattern detection algorithms
   - User influence scoring
   - Content virality analysis

3. **Storage Layer**
   - PostgreSQL for structured data and relationships
   - ElasticSearch for full-text search and analytics
   - Redis for caching and session management
   - Object storage for media files

4. **Backend Services (Spring Boot)**
   - RESTful API orchestration
   - Authentication and authorization
   - Query processing and optimization
   - Real-time notifications

5. **Frontend Dashboard (React/Flutter)**
   - Interactive timeline visualizations
   - Real-time analytics dashboards
   - Geographic filtering and mapping
   - User and content analysis tools

---

## 2. Detailed Query Workflow

### User Query Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User Input   â”‚
â”‚ â€¢ Keywords      â”‚
â”‚ â€¢ Hashtags      â”‚
â”‚ â€¢ Date Range    â”‚
â”‚ â€¢ Platforms     â”‚
â”‚ â€¢ Regions       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Frontend     â”‚
â”‚ â€¢ Validation    â”‚
â”‚ â€¢ Query Builder â”‚
â”‚ â€¢ UI State      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Spring Boot  â”‚
â”‚ â€¢ Auth Check    â”‚
â”‚ â€¢ Query Parse   â”‚
â”‚ â€¢ Route Request â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Data Query   â”‚
â”‚ â€¢ ElasticSearch â”‚
â”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ Cache Check   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. NLP Analysis â”‚
â”‚ â€¢ BERT Scoring  â”‚
â”‚ â€¢ Pattern Detectâ”‚
â”‚ â€¢ Influence Calcâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Aggregation  â”‚
â”‚ â€¢ Timeline Buildâ”‚
â”‚ â€¢ Stats Compute â”‚
â”‚ â€¢ Ranking       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Response     â”‚
â”‚ â€¢ JSON Format   â”‚
â”‚ â€¢ Visualization â”‚
â”‚ â€¢ Export Optionsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Workflow Steps

**Step 1: User Input Processing**
- Frontend validates input parameters
- Query builder constructs structured search criteria
- Real-time suggestions for keywords and hashtags

**Step 2: Authentication & Authorization**
- JWT token validation
- Role-based access control (RBAC)
- Audit logging of all queries

**Step 3: Query Optimization**
- Query parsing and optimization
- Cache lookup for similar recent queries
- Load balancing across processing nodes

**Step 4: Data Retrieval**
- ElasticSearch for full-text content search
- PostgreSQL for user relationships and metadata
- Parallel query execution for performance

**Step 5: NLP Analysis Pipeline**
- BERT sentiment scoring (positive/negative/neutral)
- Behavioral pattern classification
- User influence and reach calculations
- Content virality scoring

**Step 6: Result Aggregation**
- Chronological timeline construction
- Geographic and demographic aggregation
- Statistical analysis and trend identification
- Influencer ranking and network analysis

**Step 7: Response Formatting**
- JSON API response with structured data
- Real-time WebSocket updates for live data
- Export capabilities (PDF, CSV, JSON)

---

## 3. Technology Stack

### Data Ingestion Layer (Rust)
```rust
// Core Libraries
tokio = "1.0"           // Async runtime
reqwest = "0.11"        // HTTP client
serde = "1.0"           // Serialization
sqlx = "0.7"            // Database driver
redis = "0.23"          // Redis client
kafka = "0.8"           // Kafka producer
tracing = "0.1"         // Logging
```

**Key Components:**
- **Tokio**: Asynchronous runtime for high-concurrency
- **Reqwest**: HTTP client for API calls with retry logic
- **SQLx**: Type-safe database interactions
- **Redis**: Message queuing and caching
- **Apache Kafka**: Event streaming platform

### NLP Processing Layer (Python)
```python
# Core ML Libraries
transformers==4.35.0    # Hugging Face BERT models
torch==2.1.0           # PyTorch framework
scikit-learn==1.3.0    # ML utilities
pandas==2.1.0          # Data manipulation
numpy==1.24.0          # Numerical computing

# Infrastructure
celery==5.3.0          # Task queue
redis==5.0.0           # Message broker
psycopg2==2.9.0        # PostgreSQL driver
elasticsearch==8.10.0  # Search engine client
```

**Key Components:**
- **Transformers**: Pre-trained BERT models and fine-tuning
- **PyTorch**: Deep learning framework
- **Celery**: Distributed task processing
- **Scikit-learn**: Traditional ML algorithms

### Backend Services (Spring Boot - Java)
```xml
<!-- Core Spring Dependencies -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-security</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-websocket</artifactId>
</dependency>
```

**Key Components:**
- **Spring Security**: Authentication and authorization
- **Spring Data JPA**: Database abstraction
- **Spring WebSocket**: Real-time communication
- **Spring Cloud Gateway**: API gateway and routing

### Storage Layer
```yaml
# PostgreSQL Configuration
postgresql:
  version: "15"
  extensions:
    - postgis      # Geographic data
    - pg_trgm      # Text similarity
    - btree_gin    # Indexing

# ElasticSearch Configuration
elasticsearch:
  version: "8.10"
  plugins:
    - analysis-icu  # International text analysis
    - mapper-size   # Document size tracking

# Redis Configuration
redis:
  version: "7.2"
  modules:
    - RedisJSON    # JSON document storage
    - RedisGraph   # Graph database
```

### Frontend Layer (React)
```json
{
  "dependencies": {
    "react": "^18.2.0",
    "react-router-dom": "^6.8.0",
    "@mui/material": "^5.11.0",
    "recharts": "^2.5.0",
    "leaflet": "^1.9.0",
    "socket.io-client": "^4.6.0",
    "axios": "^1.3.0",
    "date-fns": "^2.29.0"
  }
}
```

**Key Components:**
- **Material-UI**: Component library for consistent design
- **Recharts**: Data visualization and charting
- **Leaflet**: Interactive maps for geographic data
- **Socket.IO**: Real-time data updates

---

## 4. Backend Design

### Microservices Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPRING BOOT ECOSYSTEM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Gateway   â”‚  â”‚    Auth     â”‚  â”‚   Query     â”‚             â”‚
â”‚  â”‚   Service   â”‚  â”‚   Service   â”‚  â”‚  Service    â”‚             â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚             â”‚
â”‚  â”‚ â€¢ Routing   â”‚  â”‚ â€¢ JWT       â”‚  â”‚ â€¢ Search    â”‚             â”‚
â”‚  â”‚ â€¢ Load Bal  â”‚  â”‚ â€¢ RBAC      â”‚  â”‚ â€¢ Filter    â”‚             â”‚
â”‚  â”‚ â€¢ Rate Lmt  â”‚  â”‚ â€¢ Audit     â”‚  â”‚ â€¢ Aggregate â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Analytics   â”‚  â”‚   User      â”‚  â”‚ Notificationâ”‚             â”‚
â”‚  â”‚  Service    â”‚  â”‚  Service    â”‚  â”‚   Service   â”‚             â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚             â”‚
â”‚  â”‚ â€¢ Timeline  â”‚  â”‚ â€¢ Profiles  â”‚  â”‚ â€¢ Alerts    â”‚             â”‚
â”‚  â”‚ â€¢ Trends    â”‚  â”‚ â€¢ Prefs     â”‚  â”‚ â€¢ WebSocket â”‚             â”‚
â”‚  â”‚ â€¢ Reports   â”‚  â”‚ â€¢ History   â”‚  â”‚ â€¢ Email     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Design

**REST API Endpoints:**

```java
// Query Controller
@RestController
@RequestMapping("/api/v1/queries")
public class QueryController {
    
    @PostMapping("/search")
    public ResponseEntity<SearchResults> searchContent(
        @RequestBody SearchRequest request,
        @AuthenticationPrincipal UserDetails user
    ) {
        // Implementation
    }
    
    @GetMapping("/timeline/{queryId}")
    public ResponseEntity<Timeline> getTimeline(
        @PathVariable String queryId,
        @RequestParam(defaultValue = "0") int page
    ) {
        // Implementation
    }
    
    @GetMapping("/influencers/{queryId}")
    public ResponseEntity<List<Influencer>> getInfluencers(
        @PathVariable String queryId,
        @RequestParam(defaultValue = "10") int limit
    ) {
        // Implementation
    }
}

// Analytics Controller
@RestController
@RequestMapping("/api/v1/analytics")
public class AnalyticsController {
    
    @GetMapping("/sentiment/{queryId}")
    public ResponseEntity<SentimentAnalysis> getSentimentAnalysis(
        @PathVariable String queryId,
        @RequestParam String timeframe
    ) {
        // Implementation
    }
    
    @GetMapping("/trends/{queryId}")
    public ResponseEntity<TrendAnalysis> getTrendAnalysis(
        @PathVariable String queryId
    ) {
        // Implementation
    }
}
```

### Data Models

**Core Entities:**

```java
// Social Media Post Entity
@Entity
@Table(name = "social_posts")
public class SocialPost {
    @Id
    private String id;
    
    @Column(nullable = false)
    private String platform;
    
    @Column(nullable = false)
    private String content;
    
    @Column(name = "author_id")
    private String authorId;
    
    @Column(name = "created_at")
    private LocalDateTime createdAt;
    
    @Column(name = "engagement_score")
    private Double engagementScore;
    
    @Column(name = "sentiment_score")
    private Double sentimentScore;
    
    @Column(name = "behavioral_patterns")
    @Convert(converter = JsonConverter.class)
    private List<String> behavioralPatterns;
    
    @Column(name = "geographic_data")
    @Convert(converter = JsonConverter.class)
    private GeographicData geographicData;
    
    // Getters, setters, constructors
}

// User Profile Entity
@Entity
@Table(name = "user_profiles")
public class UserProfile {
    @Id
    private String id;
    
    @Column(nullable = false)
    private String platform;
    
    @Column(name = "username")
    private String username;
    
    @Column(name = "display_name")
    private String displayName;
    
    @Column(name = "follower_count")
    private Long followerCount;
    
    @Column(name = "influence_score")
    private Double influenceScore;
    
    @Column(name = "verification_status")
    private Boolean verificationStatus;
    
    @Column(name = "account_created")
    private LocalDateTime accountCreated;
    
    // Getters, setters, constructors
}

// Query History Entity
@Entity
@Table(name = "query_history")
public class QueryHistory {
    @Id
    private String id;
    
    @Column(name = "user_id")
    private String userId;
    
    @Column(name = "query_params")
    @Convert(converter = JsonConverter.class)
    private QueryParameters queryParams;
    
    @Column(name = "created_at")
    private LocalDateTime createdAt;
    
    @Column(name = "result_count")
    private Long resultCount;
    
    @Column(name = "execution_time")
    private Long executionTime;
    
    // Getters, setters, constructors
}
```

### Database Schema

**PostgreSQL Schema:**

```sql
-- Social Posts Table
CREATE TABLE social_posts (
    id VARCHAR(255) PRIMARY KEY,
    platform VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    author_id VARCHAR(255) NOT NULL,
    created_at TIMESTAMP NOT NULL,
    engagement_score DECIMAL(10,4),
    sentiment_score DECIMAL(5,4),
    behavioral_patterns JSONB,
    geographic_data JSONB,
    indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_platform_created (platform, created_at),
    INDEX idx_author_id (author_id),
    INDEX idx_sentiment_score (sentiment_score),
    INDEX idx_geographic_data USING GIN (geographic_data)
);

-- User Profiles Table
CREATE TABLE user_profiles (
    id VARCHAR(255) PRIMARY KEY,
    platform VARCHAR(50) NOT NULL,
    username VARCHAR(255) NOT NULL,
    display_name VARCHAR(255),
    follower_count BIGINT DEFAULT 0,
    influence_score DECIMAL(10,4),
    verification_status BOOLEAN DEFAULT FALSE,
    account_created TIMESTAMP,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE INDEX idx_platform_username (platform, username),
    INDEX idx_influence_score (influence_score DESC),
    INDEX idx_follower_count (follower_count DESC)
);

-- Query History Table
CREATE TABLE query_history (
    id VARCHAR(255) PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    query_params JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    result_count BIGINT,
    execution_time BIGINT,
    
    INDEX idx_user_id_created (user_id, created_at DESC),
    INDEX idx_query_params USING GIN (query_params)
);
```

**ElasticSearch Mapping:**

```json
{
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "platform": {"type": "keyword"},
      "content": {
        "type": "text",
        "analyzer": "standard",
        "fields": {
          "keyword": {"type": "keyword"}
        }
      },
      "author_id": {"type": "keyword"},
      "created_at": {"type": "date"},
      "engagement_score": {"type": "float"},
      "sentiment_score": {"type": "float"},
      "behavioral_patterns": {"type": "keyword"},
      "geographic_data": {
        "properties": {
          "location": {"type": "geo_point"},
          "country": {"type": "keyword"},
          "region": {"type": "keyword"}
        }
      },
      "hashtags": {"type": "keyword"},
      "mentions": {"type": "keyword"}
    }
  }
}
```

---

## 5. ML/NLP Design

### BERT Fine-tuning Strategy

**Model Architecture:**

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class SentinelBERTModel(nn.Module):
    def __init__(self, bert_model_name='bert-base-uncased', num_classes=3):
        super(SentinelBERTModel, self).__init__()
        
        # Load pre-trained BERT
        self.bert = BertModel.from_pretrained(bert_model_name)
        
        # Freeze early layers, fine-tune later layers
        for param in self.bert.encoder.layer[:8].parameters():
            param.requires_grad = False
            
        # Classification heads
        self.sentiment_classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)  # positive, negative, neutral
        )
        
        # Behavioral pattern classifier
        self.behavior_classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(self.bert.config.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 10)  # 10 behavioral patterns
        )
        
        # Influence score regressor
        self.influence_regressor = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(self.bert.config.hidden_size, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, input_ids, attention_mask):
        # Get BERT embeddings
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        
        # Multi-task outputs
        sentiment_logits = self.sentiment_classifier(pooled_output)
        behavior_logits = self.behavior_classifier(pooled_output)
        influence_score = self.influence_regressor(pooled_output)
        
        return {
            'sentiment': sentiment_logits,
            'behavior': behavior_logits,
            'influence': influence_score
        }
```

**Training Pipeline:**

```python
class SentinelBERTTrainer:
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # Multi-task loss functions
        self.sentiment_criterion = nn.CrossEntropyLoss()
        self.behavior_criterion = nn.BCEWithLogitsLoss()
        self.influence_criterion = nn.MSELoss()
        
        # Optimizer with different learning rates
        self.optimizer = torch.optim.AdamW([
            {'params': self.model.bert.parameters(), 'lr': 2e-5},
            {'params': self.model.sentiment_classifier.parameters(), 'lr': 1e-4},
            {'params': self.model.behavior_classifier.parameters(), 'lr': 1e-4},
            {'params': self.model.influence_regressor.parameters(), 'lr': 1e-4}
        ])
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100
        )
    
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        
        for batch in self.train_loader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            sentiment_labels = batch['sentiment'].to(self.device)
            behavior_labels = batch['behavior'].to(self.device)
            influence_labels = batch['influence'].to(self.device)
            
            self.optimizer.zero_grad()
            
            outputs = self.model(input_ids, attention_mask)
            
            # Multi-task loss calculation
            sentiment_loss = self.sentiment_criterion(
                outputs['sentiment'], sentiment_labels
            )
            behavior_loss = self.behavior_criterion(
                outputs['behavior'], behavior_labels
            )
            influence_loss = self.influence_criterion(
                outputs['influence'].squeeze(), influence_labels
            )
            
            # Weighted combination of losses
            total_batch_loss = (
                0.4 * sentiment_loss + 
                0.4 * behavior_loss + 
                0.2 * influence_loss
            )
            
            total_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += total_batch_loss.item()
        
        self.scheduler.step()
        return total_loss / len(self.train_loader)
```

### Behavioral Pattern Detection

**Pattern Categories:**

```python
BEHAVIORAL_PATTERNS = {
    'AMPLIFICATION': {
        'description': 'Rapid sharing/retweeting behavior',
        'indicators': ['high_retweet_ratio', 'short_time_intervals', 'minimal_original_content']
    },
    'COORDINATION': {
        'description': 'Synchronized posting patterns',
        'indicators': ['similar_timing', 'identical_hashtags', 'cross_platform_sync']
    },
    'ASTROTURFING': {
        'description': 'Artificial grassroots movement',
        'indicators': ['new_accounts', 'generic_profiles', 'repetitive_messaging']
    },
    'POLARIZATION': {
        'description': 'Extreme sentiment expression',
        'indicators': ['high_sentiment_scores', 'divisive_language', 'us_vs_them_framing']
    },
    'MISINFORMATION': {
        'description': 'Spreading unverified information',
        'indicators': ['lack_of_sources', 'sensational_claims', 'rapid_viral_spread']
    }
}

class BehavioralPatternAnalyzer:
    def __init__(self):
        self.pattern_detectors = {
            'amplification': AmplificationDetector(),
            'coordination': CoordinationDetector(),
            'astroturfing': AstroturfingDetector(),
            'polarization': PolarizationDetector(),
            'misinformation': MisinformationDetector()
        }
    
    def analyze_user_behavior(self, user_posts, user_profile):
        patterns = {}
        
        for pattern_name, detector in self.pattern_detectors.items():
            score = detector.calculate_score(user_posts, user_profile)
            patterns[pattern_name] = {
                'score': score,
                'confidence': detector.get_confidence(score),
                'indicators': detector.get_indicators(user_posts, user_profile)
            }
        
        return patterns
    
    def detect_network_patterns(self, user_network):
        # Analyze patterns across user networks
        network_patterns = {}
        
        # Coordination detection across users
        coordination_score = self._detect_coordination(user_network)
        network_patterns['coordination'] = coordination_score
        
        # Amplification network analysis
        amplification_network = self._analyze_amplification_network(user_network)
        network_patterns['amplification_network'] = amplification_network
        
        return network_patterns
```

### Model Serving Strategy

**Model Deployment Architecture:**

```python
# FastAPI Model Server
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import asyncio
from typing import List, Dict

app = FastAPI(title="SentinelBERT Model Server")

class PredictionRequest(BaseModel):
    texts: List[str]
    user_metadata: Dict = None

class PredictionResponse(BaseModel):
    predictions: List[Dict]
    processing_time: float

class ModelServer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.load_model()
    
    def load_model(self):
        """Load the fine-tuned SentinelBERT model"""
        self.model = SentinelBERTModel.from_pretrained('models/sentinelbert-v1')
        self.model.to(self.device)
        self.model.eval()
        
        self.tokenizer = BertTokenizer.from_pretrained('models/sentinelbert-v1')
    
    async def predict_batch(self, texts: List[str]) -> List[Dict]:
        """Batch prediction for multiple texts"""
        # Tokenize inputs
        encoded = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # Model inference
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask)
        
        # Process outputs
        predictions = []
        for i in range(len(texts)):
            sentiment_probs = torch.softmax(outputs['sentiment'][i], dim=0)
            behavior_probs = torch.sigmoid(outputs['behavior'][i])
            influence_score = outputs['influence'][i].item()
            
            predictions.append({
                'sentiment': {
                    'positive': sentiment_probs[2].item(),
                    'negative': sentiment_probs[0].item(),
                    'neutral': sentiment_probs[1].item()
                },
                'behavioral_patterns': behavior_probs.tolist(),
                'influence_score': influence_score
            })
        
        return predictions

# Global model server instance
model_server = ModelServer()

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    start_time = time.time()
    
    try:
        predictions = await model_server.predict_batch(request.texts)
        processing_time = time.time() - start_time
        
        return PredictionResponse(
            predictions=predictions,
            processing_time=processing_time
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": model_server.model is not None}
```

**Model Versioning and A/B Testing:**

```python
class ModelManager:
    def __init__(self):
        self.models = {}
        self.current_version = "v1"
        self.ab_test_config = {}
    
    def load_model_version(self, version: str, model_path: str):
        """Load a specific model version"""
        self.models[version] = SentinelBERTModel.from_pretrained(model_path)
        self.models[version].eval()
    
    def set_ab_test(self, test_name: str, version_a: str, version_b: str, traffic_split: float):
        """Configure A/B test between model versions"""
        self.ab_test_config[test_name] = {
            'version_a': version_a,
            'version_b': version_b,
            'traffic_split': traffic_split
        }
    
    def get_model_for_request(self, request_id: str, test_name: str = None):
        """Get appropriate model version for request"""
        if test_name and test_name in self.ab_test_config:
            config = self.ab_test_config[test_name]
            # Simple hash-based traffic splitting
            if hash(request_id) % 100 < config['traffic_split'] * 100:
                return self.models[config['version_a']]
            else:
                return self.models[config['version_b']]
        
        return self.models[self.current_version]
```

---

## 6. Frontend Design

### Dashboard Architecture

**Component Hierarchy:**

```
SentinelBERT App
â”œâ”€â”€ Authentication
â”‚   â”œâ”€â”€ Login Component
â”‚   â”œâ”€â”€ Role-based Access
â”‚   â””â”€â”€ Session Management
â”œâ”€â”€ Main Dashboard
â”‚   â”œâ”€â”€ Search Interface
â”‚   â”‚   â”œâ”€â”€ Keyword Input
â”‚   â”‚   â”œâ”€â”€ Platform Selector
â”‚   â”‚   â”œâ”€â”€ Date Range Picker
â”‚   â”‚   â””â”€â”€ Region Filter
â”‚   â”œâ”€â”€ Results Display
â”‚   â”‚   â”œâ”€â”€ Timeline View
â”‚   â”‚   â”œâ”€â”€ Analytics Panel
â”‚   â”‚   â”œâ”€â”€ Influencer List
â”‚   â”‚   â””â”€â”€ Content Grid
â”‚   â””â”€â”€ Export Tools
â””â”€â”€ Admin Panel
    â”œâ”€â”€ User Management
    â”œâ”€â”€ System Monitoring
    â””â”€â”€ Model Configuration
```

### Key UI Components

**1. Search Interface:**

```jsx
import React, { useState, useCallback } from 'react';
import { 
  TextField, 
  Chip, 
  Select, 
  DatePicker, 
  Button,
  Autocomplete 
} from '@mui/material';

const SearchInterface = ({ onSearch, loading }) => {
  const [searchParams, setSearchParams] = useState({
    keywords: [],
    hashtags: [],
    platforms: ['twitter', 'instagram', 'reddit'],
    dateRange: { start: null, end: null },
    regions: [],
    sentimentFilter: 'all'
  });

  const handleSearch = useCallback(() => {
    onSearch(searchParams);
  }, [searchParams, onSearch]);

  return (
    <div className="search-interface">
      <div className="search-row">
        <Autocomplete
          multiple
          freeSolo
          options={[]}
          value={searchParams.keywords}
          onChange={(_, newValue) => 
            setSearchParams(prev => ({ ...prev, keywords: newValue }))
          }
          renderInput={(params) => (
            <TextField
              {...params}
              label="Keywords"
              placeholder="Enter keywords..."
              variant="outlined"
            />
          )}
          renderTags={(value, getTagProps) =>
            value.map((option, index) => (
              <Chip
                variant="outlined"
                label={option}
                {...getTagProps({ index })}
                key={index}
              />
            ))
          }
        />
      </div>

      <div className="search-row">
        <Select
          multiple
          value={searchParams.platforms}
          onChange={(e) => 
            setSearchParams(prev => ({ ...prev, platforms: e.target.value }))
          }
          label="Platforms"
        >
          <MenuItem value="twitter">X.com</MenuItem>
          <MenuItem value="instagram">Instagram</MenuItem>
          <MenuItem value="reddit">Reddit</MenuItem>
          <MenuItem value="facebook">Facebook</MenuItem>
        </Select>

        <DatePicker
          label="Start Date"
          value={searchParams.dateRange.start}
          onChange={(date) => 
            setSearchParams(prev => ({
              ...prev,
              dateRange: { ...prev.dateRange, start: date }
            }))
          }
        />

        <DatePicker
          label="End Date"
          value={searchParams.dateRange.end}
          onChange={(date) => 
            setSearchParams(prev => ({
              ...prev,
              dateRange: { ...prev.dateRange, end: date }
            }))
          }
        />
      </div>

      <Button
        variant="contained"
        onClick={handleSearch}
        disabled={loading}
        size="large"
      >
        {loading ? 'Searching...' : 'Search'}
      </Button>
    </div>
  );
};
```

**2. Timeline Visualization:**

```jsx
import React, { useMemo } from 'react';
import {
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
  ResponsiveContainer,
  AreaChart,
  Area
} from 'recharts';

const TimelineView = ({ data, selectedMetrics }) => {
  const chartData = useMemo(() => {
    return data.timeline.map(point => ({
      timestamp: new Date(point.timestamp).toLocaleDateString(),
      postCount: point.post_count,
      sentimentScore: point.avg_sentiment,
      engagementScore: point.avg_engagement,
      influenceScore: point.total_influence
    }));
  }, [data]);

  return (
    <div className="timeline-view">
      <div className="timeline-header">
        <h3>Content Timeline</h3>
        <div className="metric-toggles">
          {['postCount', 'sentimentScore', 'engagementScore'].map(metric => (
            <Chip
              key={metric}
              label={metric}
              clickable
              color={selectedMetrics.includes(metric) ? 'primary' : 'default'}
              onClick={() => toggleMetric(metric)}
            />
          ))}
        </div>
      </div>

      <ResponsiveContainer width="100%" height={400}>
        <LineChart data={chartData}>
          <CartesianGrid strokeDasharray="3 3" />
          <XAxis dataKey="timestamp" />
          <YAxis />
          <Tooltip />
          <Legend />
          
          {selectedMetrics.includes('postCount') && (
            <Line
              type="monotone"
              dataKey="postCount"
              stroke="#8884d8"
              strokeWidth={2}
              name="Post Count"
            />
          )}
          
          {selectedMetrics.includes('sentimentScore') && (
            <Line
              type="monotone"
              dataKey="sentimentScore"
              stroke="#82ca9d"
              strokeWidth={2}
              name="Sentiment Score"
            />
          )}
          
          {selectedMetrics.includes('engagementScore') && (
            <Line
              type="monotone"
              dataKey="engagementScore"
              stroke="#ffc658"
              strokeWidth={2}
              name="Engagement Score"
            />
          )}
        </LineChart>
      </ResponsiveContainer>

      <div className="timeline-insights">
        <div className="insight-card">
          <h4>Peak Activity</h4>
          <p>{data.insights.peak_time}</p>
        </div>
        <div className="insight-card">
          <h4>Trend Direction</h4>
          <p>{data.insights.trend_direction}</p>
        </div>
        <div className="insight-card">
          <h4>Viral Threshold</h4>
          <p>{data.insights.viral_threshold}</p>
        </div>
      </div>
    </div>
  );
};
```

**3. Influencer Analysis Panel:**

```jsx
import React from 'react';
import {
  Table,
  TableBody,
  TableCell,
  TableContainer,
  TableHead,
  TableRow,
  Avatar,
  Chip,
  LinearProgress,
  IconButton
} from '@mui/material';
import { Visibility, Share, Flag } from '@mui/icons-material';

const InfluencerPanel = ({ influencers, onViewProfile, onFlag }) => {
  return (
    <div className="influencer-panel">
      <div className="panel-header">
        <h3>Key Influencers</h3>
        <div className="sort-controls">
          <Select value="influence" onChange={handleSortChange}>
            <MenuItem value="influence">Influence Score</MenuItem>
            <MenuItem value="reach">Reach</MenuItem>
            <MenuItem value="engagement">Engagement</MenuItem>
          </Select>
        </div>
      </div>

      <TableContainer>
        <Table>
          <TableHead>
            <TableRow>
              <TableCell>User</TableCell>
              <TableCell>Platform</TableCell>
              <TableCell>Influence Score</TableCell>
              <TableCell>Reach</TableCell>
              <TableCell>Sentiment</TableCell>
              <TableCell>Behavioral Patterns</TableCell>
              <TableCell>Actions</TableCell>
            </TableRow>
          </TableHead>
          <TableBody>
            {influencers.map((influencer) => (
              <TableRow key={influencer.id}>
                <TableCell>
                  <div className="user-cell">
                    <Avatar src={influencer.avatar} />
                    <div>
                      <div className="username">{influencer.username}</div>
                      <div className="display-name">{influencer.displayName}</div>
                    </div>
                  </div>
                </TableCell>
                
                <TableCell>
                  <Chip
                    label={influencer.platform}
                    size="small"
                    color="primary"
                  />
                </TableCell>
                
                <TableCell>
                  <div className="score-cell">
                    <LinearProgress
                      variant="determinate"
                      value={influencer.influenceScore * 100}
                      className="score-bar"
                    />
                    <span>{(influencer.influenceScore * 100).toFixed(1)}%</span>
                  </div>
                </TableCell>
                
                <TableCell>
                  {influencer.reach.toLocaleString()}
                </TableCell>
                
                <TableCell>
                  <Chip
                    label={influencer.dominantSentiment}
                    color={getSentimentColor(influencer.dominantSentiment)}
                    size="small"
                  />
                </TableCell>
                
                <TableCell>
                  <div className="patterns-cell">
                    {influencer.behavioralPatterns.slice(0, 2).map(pattern => (
                      <Chip
                        key={pattern}
                        label={pattern}
                        size="small"
                        variant="outlined"
                      />
                    ))}
                    {influencer.behavioralPatterns.length > 2 && (
                      <span>+{influencer.behavioralPatterns.length - 2}</span>
                    )}
                  </div>
                </TableCell>
                
                <TableCell>
                  <IconButton onClick={() => onViewProfile(influencer.id)}>
                    <Visibility />
                  </IconButton>
                  <IconButton>
                    <Share />
                  </IconButton>
                  <IconButton onClick={() => onFlag(influencer.id)}>
                    <Flag />
                  </IconButton>
                </TableCell>
              </TableRow>
            ))}
          </TableBody>
        </Table>
      </TableContainer>
    </div>
  );
};
```

**4. Geographic Analysis:**

```jsx
import React, { useEffect, useRef } from 'react';
import L from 'leaflet';
import 'leaflet/dist/leaflet.css';

const GeographicMap = ({ data, onRegionSelect }) => {
  const mapRef = useRef(null);
  const mapInstanceRef = useRef(null);

  useEffect(() => {
    if (!mapInstanceRef.current) {
      // Initialize map
      mapInstanceRef.current = L.map(mapRef.current).setView([40.7128, -74.0060], 4);
      
      // Add tile layer
      L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
        attribution: 'Â© OpenStreetMap contributors'
      }).addTo(mapInstanceRef.current);
    }

    // Clear existing markers
    mapInstanceRef.current.eachLayer((layer) => {
      if (layer instanceof L.Marker) {
        mapInstanceRef.current.removeLayer(layer);
      }
    });

    // Add new markers
    data.regions.forEach(region => {
      const marker = L.marker([region.lat, region.lng])
        .addTo(mapInstanceRef.current)
        .bindPopup(`
          <div class="map-popup">
            <h4>${region.name}</h4>
            <p>Posts: ${region.postCount}</p>
            <p>Avg Sentiment: ${region.avgSentiment.toFixed(2)}</p>
            <p>Top Keywords: ${region.topKeywords.join(', ')}</p>
          </div>
        `);

      marker.on('click', () => onRegionSelect(region));
    });

  }, [data, onRegionSelect]);

  return (
    <div className="geographic-map">
      <div className="map-header">
        <h3>Geographic Distribution</h3>
        <div className="map-controls">
          <Select value="sentiment" onChange={handleMetricChange}>
            <MenuItem value="sentiment">Sentiment</MenuItem>
            <MenuItem value="volume">Volume</MenuItem>
            <MenuItem value="engagement">Engagement</MenuItem>
          </Select>
        </div>
      </div>
      <div ref={mapRef} className="map-container" style={{ height: '400px' }} />
    </div>
  );
};
```

### Real-time Updates

**WebSocket Integration:**

```jsx
import React, { useEffect, useState } from 'react';
import io from 'socket.io-client';

const RealTimeUpdates = ({ queryId, onUpdate }) => {
  const [socket, setSocket] = useState(null);
  const [connectionStatus, setConnectionStatus] = useState('disconnected');

  useEffect(() => {
    // Initialize WebSocket connection
    const newSocket = io(process.env.REACT_APP_WEBSOCKET_URL, {
      auth: {
        token: localStorage.getItem('authToken')
      }
    });

    newSocket.on('connect', () => {
      setConnectionStatus('connected');
      // Subscribe to query updates
      newSocket.emit('subscribe', { queryId });
    });

    newSocket.on('disconnect', () => {
      setConnectionStatus('disconnected');
    });

    newSocket.on('query_update', (data) => {
      onUpdate(data);
    });

    newSocket.on('new_content', (content) => {
      // Handle new content matching the query
      onUpdate({ type: 'new_content', data: content });
    });

    newSocket.on('sentiment_update', (sentimentData) => {
      // Handle sentiment analysis updates
      onUpdate({ type: 'sentiment_update', data: sentimentData });
    });

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, [queryId, onUpdate]);

  return (
    <div className="real-time-status">
      <div className={`status-indicator ${connectionStatus}`}>
        {connectionStatus === 'connected' ? 'ğŸŸ¢' : 'ğŸ”´'} Live Updates
      </div>
    </div>
  );
};
```

---

## 7. Deployment Plan

### Containerization Strategy

**Docker Configuration:**

```dockerfile
# Rust Ingestion Service
FROM rust:1.70 as builder
WORKDIR /app
COPY Cargo.toml Cargo.lock ./
COPY src ./src
RUN cargo build --release

FROM debian:bullseye-slim
RUN apt-get update && apt-get install -y \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*
COPY --from=builder /app/target/release/sentinel-ingestion /usr/local/bin/
EXPOSE 8080
CMD ["sentinel-ingestion"]

# Python NLP Service
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

# Spring Boot Backend
FROM openjdk:17-jdk-slim
WORKDIR /app
COPY target/sentinel-backend-*.jar app.jar
EXPOSE 8080
CMD ["java", "-jar", "app.jar"]

# React Frontend
FROM node:18-alpine as build
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

**Docker Compose for Development:**

```yaml
version: '3.8'

services:
  # Database Services
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: sentinelbert
      POSTGRES_USER: sentinel
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  elasticsearch:
    image: elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  redis:
    image: redis:7.2-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # Message Queue
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # Application Services
  ingestion-service:
    build:
      context: ./services/ingestion
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=postgresql://sentinel:${POSTGRES_PASSWORD}@postgres:5432/sentinelbert
      - REDIS_URL=redis://redis:6379
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - postgres
      - redis
      - kafka
    ports:
      - "8081:8080"

  nlp-service:
    build:
      context: ./services/nlp
      dockerfile: Dockerfile
    environment:
      - MODEL_PATH=/app/models
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./models:/app/models
    depends_on:
      - redis
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  backend-service:
    build:
      context: ./services/backend
      dockerfile: Dockerfile
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/sentinelbert
      - SPRING_DATASOURCE_USERNAME=sentinel
      - SPRING_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD}
      - SPRING_REDIS_HOST=redis
      - ELASTICSEARCH_HOST=elasticsearch
    depends_on:
      - postgres
      - elasticsearch
      - redis
    ports:
      - "8080:8080"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    environment:
      - REACT_APP_API_URL=http://localhost:8080
      - REACT_APP_WEBSOCKET_URL=ws://localhost:8080
    ports:
      - "3000:80"
    depends_on:
      - backend-service

volumes:
  postgres_data:
  elasticsearch_data:
  redis_data:
```

### Kubernetes Deployment

**Kubernetes Manifests:**

```yaml
# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: sentinelbert

---
# ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: sentinelbert-config
  namespace: sentinelbert
data:
  postgres-host: "postgres-service"
  elasticsearch-host: "elasticsearch-service"
  redis-host: "redis-service"
  kafka-brokers: "kafka-service:9092"

---
# PostgreSQL Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: sentinelbert
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_DB
          value: "sentinelbert"
        - name: POSTGRES_USER
          value: "sentinel"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc

---
# Ingestion Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingestion-service
  namespace: sentinelbert
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ingestion-service
  template:
    metadata:
      labels:
        app: ingestion-service
    spec:
      containers:
      - name: ingestion
        image: sentinelbert/ingestion:latest
        env:
        - name: DATABASE_URL
          value: "postgresql://sentinel:$(POSTGRES_PASSWORD)@postgres-service:5432/sentinelbert"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: KAFKA_BROKERS
          value: "kafka-service:9092"
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

---
# NLP Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nlp-service
  namespace: sentinelbert
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nlp-service
  template:
    metadata:
      labels:
        app: nlp-service
    spec:
      containers:
      - name: nlp
        image: sentinelbert/nlp:latest
        env:
        - name: MODEL_PATH
          value: "/app/models"
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-storage
          mountPath: /app/models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc

---
# Backend Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-service
  namespace: sentinelbert
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend-service
  template:
    metadata:
      labels:
        app: backend-service
    spec:
      containers:
      - name: backend
        image: sentinelbert/backend:latest
        env:
        - name: SPRING_DATASOURCE_URL
          value: "jdbc:postgresql://postgres-service:5432/sentinelbert"
        - name: SPRING_DATASOURCE_USERNAME
          value: "sentinel"
        - name: SPRING_DATASOURCE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: SPRING_REDIS_HOST
          value: "redis-service"
        - name: ELASTICSEARCH_HOST
          value: "elasticsearch-service"
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

---
# Ingress Configuration
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sentinelbert-ingress
  namespace: sentinelbert
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - sentinelbert.example.com
    secretName: sentinelbert-tls
  rules:
  - host: sentinelbert.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: backend-service
            port:
              number: 8080
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

### CI/CD Pipeline

**GitHub Actions Workflow:**

```yaml
name: SentinelBERT CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Set up Java
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
        
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        
    # Rust Tests
    - name: Run Rust tests
      working-directory: ./services/ingestion
      run: |
        cargo test
        
    # Python Tests
    - name: Install Python dependencies
      working-directory: ./services/nlp
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
        
    - name: Run Python tests
      working-directory: ./services/nlp
      run: |
        pytest --cov=. --cov-report=xml
        
    # Java Tests
    - name: Run Java tests
      working-directory: ./services/backend
      run: |
        ./mvnw test
        
    # Frontend Tests
    - name: Install Node dependencies
      working-directory: ./frontend
      run: npm ci
      
    - name: Run Frontend tests
      working-directory: ./frontend
      run: npm test -- --coverage --watchAll=false

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    strategy:
      matrix:
        service: [ingestion, nlp, backend, frontend]
        
    steps:
    - uses: actions/checkout@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}
        
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: ./services/${{ matrix.service }}
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
        
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
        
    - name: Deploy to Kubernetes
      run: |
        kubectl apply -f k8s/
        kubectl rollout restart deployment/ingestion-service -n sentinelbert
        kubectl rollout restart deployment/nlp-service -n sentinelbert
        kubectl rollout restart deployment/backend-service -n sentinelbert
        kubectl rollout restart deployment/frontend -n sentinelbert
        
    - name: Verify deployment
      run: |
        kubectl rollout status deployment/ingestion-service -n sentinelbert
        kubectl rollout status deployment/nlp-service -n sentinelbert
        kubectl rollout status deployment/backend-service -n sentinelbert
        kubectl rollout status deployment/frontend -n sentinelbert
```

---

## 8. Security & Privacy Measures

### Authentication & Authorization

**Multi-layered Security Architecture:**

```java
@Configuration
@EnableWebSecurity
@EnableGlobalMethodSecurity(prePostEnabled = true)
public class SecurityConfig {

    @Autowired
    private JwtAuthenticationEntryPoint jwtAuthenticationEntryPoint;

    @Autowired
    private JwtRequestFilter jwtRequestFilter;

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder(12);
    }

    @Bean
    public AuthenticationManager authenticationManager(
            AuthenticationConfiguration config) throws Exception {
        return config.getAuthenticationManager();
    }

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http.csrf().disable()
            .authorizeHttpRequests(authz -> authz
                .requestMatchers("/api/auth/**").permitAll()
                .requestMatchers("/api/admin/**").hasRole("ADMIN")
                .requestMatchers("/api/analyst/**").hasAnyRole("ANALYST", "ADMIN")
                .requestMatchers("/api/viewer/**").hasAnyRole("VIEWER", "ANALYST", "ADMIN")
                .anyRequest().authenticated()
            )
            .exceptionHandling().authenticationEntryPoint(jwtAuthenticationEntryPoint)
            .and()
            .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS);

        http.addFilterBefore(jwtRequestFilter, UsernamePasswordAuthenticationFilter.class);
        
        // Additional security headers
        http.headers(headers -> headers
            .frameOptions().deny()
            .contentTypeOptions().and()
            .httpStrictTransportSecurity(hstsConfig -> hstsConfig
                .maxAgeInSeconds(31536000)
                .includeSubdomains(true)
            )
        );

        return http.build();
    }
}

// Role-based Access Control
@Entity
@Table(name = "users")
public class User {
    @Id
    private String id;
    
    @Column(unique = true, nullable = false)
    private String username;
    
    @Column(nullable = false)
    private String passwordHash;
    
    @Enumerated(EnumType.STRING)
    private Role role;
    
    @Column(name = "department")
    private String department;
    
    @Column(name = "clearance_level")
    private Integer clearanceLevel;
    
    @Column(name = "last_login")
    private LocalDateTime lastLogin;
    
    @Column(name = "failed_login_attempts")
    private Integer failedLoginAttempts;
    
    @Column(name = "account_locked")
    private Boolean accountLocked;
    
    // Getters, setters, constructors
}

public enum Role {
    VIEWER("Can view search results and basic analytics"),
    ANALYST("Can perform advanced searches and export data"),
    ADMIN("Full system access including user management"),
    SUPER_ADMIN("System administration and configuration");
    
    private final String description;
    
    Role(String description) {
        this.description = description;
    }
}
```

### Data Encryption & Privacy

**Encryption Strategy:**

```java
@Service
public class EncryptionService {
    
    private final AESUtil aesUtil;
    private final RSAUtil rsaUtil;
    
    // Field-level encryption for sensitive data
    @EventListener
    public void handlePrePersist(PrePersistEvent event) {
        Object entity = event.getEntity();
        
        if (entity instanceof SocialPost) {
            SocialPost post = (SocialPost) entity;
            
            // Encrypt PII data
            if (post.getAuthorEmail() != null) {
                post.setAuthorEmail(aesUtil.encrypt(post.getAuthorEmail()));
            }
            
            // Hash user identifiers
            if (post.getAuthorId() != null) {
                post.setAuthorIdHash(hashUserId(post.getAuthorId()));
                post.setAuthorId(null); // Remove original ID
            }
        }
    }
    
    // Data anonymization for analytics
    public AnonymizedPost anonymizePost(SocialPost post) {
        return AnonymizedPost.builder()
            .id(generateAnonymousId(post.getId()))
            .content(sanitizeContent(post.getContent()))
            .timestamp(post.getCreatedAt())
            .platform(post.getPlatform())
            .sentimentScore(post.getSentimentScore())
            .geographicRegion(generalizeLocation(post.getGeographicData()))
            .build();
    }
    
    private String hashUserId(String userId) {
        return DigestUtils.sha256Hex(userId + getSystemSalt());
    }
    
    private String sanitizeContent(String content) {
        // Remove or mask PII from content
        return content
            .replaceAll("\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "[EMAIL]")
            .replaceAll("\\b\\d{3}-\\d{2}-\\d{4}\\b", "[SSN]")
            .replaceAll("\\b\\d{3}-\\d{3}-\\d{4}\\b", "[PHONE]");
    }
}

// Database encryption configuration
@Configuration
public class DatabaseEncryptionConfig {
    
    @Bean
    public AttributeConverter<String, String> stringEncryptor() {
        return new StringEncryptor();
    }
    
    @Converter
    public static class StringEncryptor implements AttributeConverter<String, String> {
        
        private final AESUtil aesUtil = new AESUtil();
        
        @Override
        public String convertToDatabaseColumn(String attribute) {
            if (attribute == null) return null;
            return aesUtil.encrypt(attribute);
        }
        
        @Override
        public String convertToEntityAttribute(String dbData) {
            if (dbData == null) return null;
            return aesUtil.decrypt(dbData);
        }
    }
}
```

### Audit Logging & Compliance

**Comprehensive Audit System:**

```java
@Entity
@Table(name = "audit_logs")
public class AuditLog {
    @Id
    private String id;
    
    @Column(name = "user_id", nullable = false)
    private String userId;
    
    @Column(name = "action", nullable = false)
    private String action;
    
    @Column(name = "resource_type")
    private String resourceType;
    
    @Column(name = "resource_id")
    private String resourceId;
    
    @Column(name = "timestamp", nullable = false)
    private LocalDateTime timestamp;
    
    @Column(name = "ip_address")
    private String ipAddress;
    
    @Column(name = "user_agent")
    private String userAgent;
    
    @Column(name = "request_details")
    @Convert(converter = JsonConverter.class)
    private Map<String, Object> requestDetails;
    
    @Column(name = "result")
    private String result; // SUCCESS, FAILURE, UNAUTHORIZED
    
    // Getters, setters, constructors
}

@Component
@Aspect
public class AuditAspect {
    
    @Autowired
    private AuditService auditService;
    
    @Around("@annotation(Auditable)")
    public Object auditMethod(ProceedingJoinPoint joinPoint) throws Throwable {
        Auditable auditable = getAuditableAnnotation(joinPoint);
        
        String userId = getCurrentUserId();
        String action = auditable.action();
        String resourceType = auditable.resourceType();
        
        AuditLog auditLog = AuditLog.builder()
            .userId(userId)
            .action(action)
            .resourceType(resourceType)
            .timestamp(LocalDateTime.now())
            .ipAddress(getCurrentUserIP())
            .userAgent(getCurrentUserAgent())
            .build();
        
        try {
            Object result = joinPoint.proceed();
            auditLog.setResult("SUCCESS");
            auditLog.setRequestDetails(extractRequestDetails(joinPoint.getArgs()));
            return result;
        } catch (Exception e) {
            auditLog.setResult("FAILURE");
            auditLog.setRequestDetails(Map.of("error", e.getMessage()));
            throw e;
        } finally {
            auditService.logAudit(auditLog);
        }
    }
}

// Usage example
@RestController
public class QueryController {
    
    @PostMapping("/search")
    @Auditable(action = "SEARCH_QUERY", resourceType = "SOCIAL_POSTS")
    public ResponseEntity<SearchResults> searchContent(@RequestBody SearchRequest request) {
        // Implementation
    }
    
    @GetMapping("/export/{queryId}")
    @Auditable(action = "DATA_EXPORT", resourceType = "QUERY_RESULTS")
    public ResponseEntity<byte[]> exportResults(@PathVariable String queryId) {
        // Implementation
    }
}
```

### Network Security & API Protection

**API Security Configuration:**

```java
@Configuration
public class ApiSecurityConfig {
    
    // Rate limiting
    @Bean
    public RedisRateLimiter rateLimiter() {
        return new RedisRateLimiter(10, 20, 1); // 10 requests per second, burst of 20
    }
    
    // API key validation
    @Component
    public class ApiKeyFilter implements Filter {
        
        @Override
        public void doFilter(ServletRequest request, ServletResponse response, 
                           FilterChain chain) throws IOException, ServletException {
            
            HttpServletRequest httpRequest = (HttpServletRequest) request;
            String apiKey = httpRequest.getHeader("X-API-Key");
            
            if (isPublicEndpoint(httpRequest.getRequestURI())) {
                chain.doFilter(request, response);
                return;
            }
            
            if (!isValidApiKey(apiKey)) {
                HttpServletResponse httpResponse = (HttpServletResponse) response;
                httpResponse.setStatus(HttpStatus.UNAUTHORIZED.value());
                return;
            }
            
            chain.doFilter(request, response);
        }
        
        private boolean isValidApiKey(String apiKey) {
            // Validate API key against database
            return apiKeyService.validateKey(apiKey);
        }
    }
    
    // Input validation and sanitization
    @ControllerAdvice
    public class InputValidationAdvice {
        
        @InitBinder
        public void initBinder(WebDataBinder binder) {
            binder.registerCustomEditor(String.class, new StringTrimmerEditor(true));
            binder.addValidators(new XSSValidator());
        }
    }
    
    public class XSSValidator implements Validator {
        
        private final Pattern[] xssPatterns = {
            Pattern.compile("<script>(.*?)</script>", Pattern.CASE_INSENSITIVE),
            Pattern.compile("src[\r\n]*=[\r\n]*\\\'(.*?)\\\'", Pattern.CASE_INSENSITIVE),
            Pattern.compile("javascript:", Pattern.CASE_INSENSITIVE),
            Pattern.compile("vbscript:", Pattern.CASE_INSENSITIVE)
        };
        
        @Override
        public boolean supports(Class<?> clazz) {
            return String.class.equals(clazz);
        }
        
        @Override
        public void validate(Object target, Errors errors) {
            String input = (String) target;
            if (input != null) {
                for (Pattern pattern : xssPatterns) {
                    if (pattern.matcher(input).find()) {
                        errors.reject("xss.detected", "Potential XSS attack detected");
                        break;
                    }
                }
            }
        }
    }
}
```

### Data Retention & Privacy Compliance

**GDPR/Privacy Compliance:**

```java
@Service
public class DataRetentionService {
    
    @Scheduled(cron = "0 0 2 * * ?") // Daily at 2 AM
    public void enforceDataRetention() {
        
        // Delete old social posts (configurable retention period)
        LocalDateTime cutoffDate = LocalDateTime.now().minusDays(getRetentionDays());
        
        List<SocialPost> expiredPosts = socialPostRepository
            .findByCreatedAtBefore(cutoffDate);
            
        for (SocialPost post : expiredPosts) {
            // Archive to cold storage before deletion
            archiveService.archivePost(post);
            
            // Delete from active database
            socialPostRepository.delete(post);
            
            // Log retention action
            auditService.logDataRetention(post.getId(), "DELETED_EXPIRED");
        }
        
        // Anonymize old user data
        anonymizeOldUserData(cutoffDate);
        
        // Clean up temporary files
        cleanupTempFiles();
    }
    
    @Async
    public CompletableFuture<Void> handleDataDeletionRequest(String userId, String reason) {
        try {
            // Find all data associated with user
            List<SocialPost> userPosts = socialPostRepository.findByAuthorIdHash(
                hashService.hashUserId(userId)
            );
            
            List<QueryHistory> userQueries = queryHistoryRepository.findByUserId(userId);
            
            // Delete user data
            socialPostRepository.deleteAll(userPosts);
            queryHistoryRepository.deleteAll(userQueries);
            
            // Log deletion
            auditService.logDataDeletion(userId, reason, userPosts.size() + userQueries.size());
            
            // Notify user of completion
            notificationService.sendDeletionConfirmation(userId);
            
        } catch (Exception e) {
            log.error("Failed to process data deletion request for user: {}", userId, e);
            notificationService.sendDeletionError(userId);
        }
        
        return CompletableFuture.completedFuture(null);
    }
    
    public DataExportPackage exportUserData(String userId) {
        // Collect all user data
        UserProfile profile = userRepository.findById(userId).orElse(null);
        List<QueryHistory> queries = queryHistoryRepository.findByUserId(userId);
        List<AuditLog> auditLogs = auditLogRepository.findByUserId(userId);
        
        // Create export package
        return DataExportPackage.builder()
            .profile(profile)
            .queryHistory(queries)
            .auditHistory(auditLogs)
            .exportDate(LocalDateTime.now())
            .build();
    }
}
```

---

## 9. Scalability & Future Features

### Horizontal Scaling Strategy

**Auto-scaling Configuration:**

```yaml
# Kubernetes Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ingestion-service-hpa
  namespace: sentinelbert
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ingestion-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: kafka_consumer_lag
      target:
        type: AverageValue
        averageValue: "100"

---
# Vertical Pod Autoscaler for NLP Service
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nlp-service-vpa
  namespace: sentinelbert
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nlp-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: nlp
      maxAllowed:
        cpu: 8
        memory: 16Gi
      minAllowed:
        cpu: 2
        memory: 4Gi
```

**Database Scaling:**

```sql
-- PostgreSQL Partitioning Strategy
CREATE TABLE social_posts_y2024m01 PARTITION OF social_posts
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE social_posts_y2024m02 PARTITION OF social_posts
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- Read Replicas Configuration
CREATE PUBLICATION social_posts_pub FOR TABLE social_posts;

-- Automated partition management
CREATE OR REPLACE FUNCTION create_monthly_partitions()
RETURNS void AS $$
DECLARE
    start_date date;
    end_date date;
    partition_name text;
BEGIN
    start_date := date_trunc('month', CURRENT_DATE + interval '1 month');
    end_date := start_date + interval '1 month';
    partition_name := 'social_posts_y' || to_char(start_date, 'YYYY') || 'm' || to_char(start_date, 'MM');
    
    EXECUTE format('CREATE TABLE %I PARTITION OF social_posts FOR VALUES FROM (%L) TO (%L)',
                   partition_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;

-- Schedule partition creation
SELECT cron.schedule('create-partitions', '0 0 1 * *', 'SELECT create_monthly_partitions();');
```

### Performance Optimization

**Caching Strategy:**

```java
@Configuration
@EnableCaching
public class CacheConfig {
    
    @Bean
    public CacheManager cacheManager() {
        RedisCacheManager.Builder builder = RedisCacheManager
            .RedisCacheManagerBuilder
            .fromConnectionFactory(redisConnectionFactory())
            .cacheDefaults(cacheConfiguration());
            
        return builder.build();
    }
    
    private RedisCacheConfiguration cacheConfiguration() {
        return RedisCacheConfiguration.defaultCacheConfig()
            .entryTtl(Duration.ofMinutes(30))
            .serializeKeysWith(RedisSerializationContext.SerializationPair
                .fromSerializer(new StringRedisSerializer()))
            .serializeValuesWith(RedisSerializationContext.SerializationPair
                .fromSerializer(new GenericJackson2JsonRedisSerializer()));
    }
    
    // Multi-level caching
    @Bean
    public CacheManager multiLevelCacheManager() {
        CompositeCacheManager cacheManager = new CompositeCacheManager();
        
        // L1 Cache: In-memory (Caffeine)
        CaffeineCacheManager caffeineCacheManager = new CaffeineCacheManager();
        caffeineCacheManager.setCaffeine(Caffeine.newBuilder()
            .maximumSize(10000)
            .expireAfterWrite(5, TimeUnit.MINUTES));
            
        // L2 Cache: Redis
        RedisCacheManager redisCacheManager = RedisCacheManager
            .builder(redisConnectionFactory())
            .cacheDefaults(cacheConfiguration())
            .build();
            
        cacheManager.setCacheManagers(Arrays.asList(
            caffeineCacheManager, 
            redisCacheManager
        ));
        
        return cacheManager;
    }
}

@Service
public class QueryCacheService {
    
    @Cacheable(value = "search-results", key = "#request.hashCode()")
    public SearchResults getCachedSearchResults(SearchRequest request) {
        return performSearch(request);
    }
    
    @Cacheable(value = "sentiment-analysis", key = "#postId")
    public SentimentAnalysis getCachedSentimentAnalysis(String postId) {
        return performSentimentAnalysis(postId);
    }
    
    @CacheEvict(value = "search-results", allEntries = true)
    @Scheduled(fixedRate = 3600000) // Clear cache every hour
    public void clearSearchCache() {
        log.info("Clearing search results cache");
    }
}
```

### Future Feature Roadmap

**Phase 1: Enhanced Media Analysis (6 months)**

```python
# Image Sentiment Analysis
class ImageSentimentAnalyzer:
    def __init__(self):
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.emotion_classifier = pipeline("image-classification", 
                                         model="j-hartmann/emotion-english-distilroberta-base")
    
    def analyze_image_sentiment(self, image_url: str) -> Dict:
        # Download and process image
        image = self.download_image(image_url)
        
        # CLIP-based sentiment analysis
        text_queries = [
            "a happy positive image",
            "a sad negative image", 
            "a neutral image",
            "an angry aggressive image"
        ]
        
        inputs = self.clip_processor(
            text=text_queries, 
            images=image, 
            return_tensors="pt", 
            padding=True
        )
        
        outputs = self.clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
        
        # Face emotion detection
        faces = self.detect_faces(image)
        face_emotions = []
        for face in faces:
            emotion = self.emotion_classifier(face)
            face_emotions.append(emotion)
        
        return {
            'overall_sentiment': {
                'positive': probs[0][0].item(),
                'negative': probs[0][1].item(),
                'neutral': probs[0][2].item(),
                'aggressive': probs[0][3].item()
            },
            'face_emotions': face_emotions,
            'confidence': float(torch.max(probs).item())
        }

# Video Analysis Pipeline
class VideoSentimentAnalyzer:
    def __init__(self):
        self.audio_analyzer = AudioSentimentAnalyzer()
        self.image_analyzer = ImageSentimentAnalyzer()
        self.text_analyzer = SentinelBERTModel()
    
    async def analyze_video_sentiment(self, video_url: str) -> Dict:
        # Extract components
        audio_path = await self.extract_audio(video_url)
        frames = await self.extract_frames(video_url, interval=1.0)
        transcript = await self.extract_transcript(audio_path)
        
        # Parallel analysis
        tasks = [
            self.audio_analyzer.analyze(audio_path),
            self.analyze_frames(frames),
            self.text_analyzer.analyze(transcript)
        ]
        
        audio_sentiment, visual_sentiment, text_sentiment = await asyncio.gather(*tasks)
        
        # Weighted combination
        combined_sentiment = self.combine_multimodal_sentiment(
            audio_sentiment, visual_sentiment, text_sentiment
        )
        
        return {
            'combined_sentiment': combined_sentiment,
            'audio_sentiment': audio_sentiment,
            'visual_sentiment': visual_sentiment,
            'text_sentiment': text_sentiment,
            'temporal_analysis': self.analyze_temporal_patterns(frames, transcript)
        }
```

**Phase 2: Predictive Analytics (12 months)**

```python
# Trend Prediction Model
class TrendPredictionModel:
    def __init__(self):
        self.lstm_model = self.build_lstm_model()
        self.feature_extractor = TrendFeatureExtractor()
        
    def build_lstm_model(self):
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(24, 50)),  # 24 hours, 50 features
            Dropout(0.2),
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            LSTM(32),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')  # Probability of going viral
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def predict_viral_potential(self, content_data: Dict) -> Dict:
        # Extract features
        features = self.feature_extractor.extract_features(content_data)
        
        # Predict viral probability
        viral_prob = self.lstm_model.predict(features.reshape(1, 24, 50))[0][0]
        
        # Predict peak time
        peak_time = self.predict_peak_time(features)
        
        # Predict reach
        estimated_reach = self.predict_reach(features, viral_prob)
        
        return {
            'viral_probability': float(viral_prob),
            'predicted_peak_time': peak_time,
            'estimated_reach': estimated_reach,
            'confidence_interval': self.calculate_confidence_interval(features),
            'key_factors': self.identify_key_factors(features)
        }

# Real-time Alert System
class AlertSystem:
    def __init__(self):
        self.alert_rules = self.load_alert_rules()
        self.notification_service = NotificationService()
        
    async def monitor_trends(self):
        while True:
            current_trends = await self.get_current_trends()
            
            for trend in current_trends:
                alerts = self.evaluate_alerts(trend)
                
                for alert in alerts:
                    await self.send_alert(alert)
            
            await asyncio.sleep(60)  # Check every minute
    
    def evaluate_alerts(self, trend: Dict) -> List[Alert]:
        alerts = []
        
        # Viral content alert
        if trend['viral_probability'] > 0.8:
            alerts.append(Alert(
                type='VIRAL_CONTENT',
                severity='HIGH',
                message=f"Content likely to go viral: {trend['content_preview']}",
                data=trend
            ))
        
        # Sentiment shift alert
        if abs(trend['sentiment_change']) > 0.5:
            alerts.append(Alert(
                type='SENTIMENT_SHIFT',
                severity='MEDIUM',
                message=f"Significant sentiment shift detected: {trend['sentiment_change']}",
                data=trend
            ))
        
        # Coordinated behavior alert
        if trend['coordination_score'] > 0.7:
            alerts.append(Alert(
                type='COORDINATED_BEHAVIOR',
                severity='HIGH',
                message=f"Potential coordinated behavior detected",
                data=trend
            ))
        
        return alerts
```

**Phase 3: Advanced Analytics (18 months)**

```python
# Network Analysis and Community Detection
class SocialNetworkAnalyzer:
    def __init__(self):
        self.graph = nx.Graph()
        self.community_detector = CommunityDetector()
        
    def build_interaction_network(self, posts: List[SocialPost]) -> nx.Graph:
        # Build graph from interactions
        for post in posts:
            author = post.author_id
            
            # Add nodes
            self.graph.add_node(author, **post.author_metadata)
            
            # Add edges for interactions
            for interaction in post.interactions:
                if interaction.type in ['reply', 'retweet', 'mention']:
                    self.graph.add_edge(
                        author, 
                        interaction.target_user,
                        weight=interaction.strength,
                        type=interaction.type
                    )
        
        return self.graph
    
    def detect_influence_networks(self) -> Dict:
        # Community detection
        communities = self.community_detector.detect_communities(self.graph)
        
        # Influence scoring
        influence_scores = nx.eigenvector_centrality(self.graph)
        betweenness_scores = nx.betweenness_centrality(self.graph)
        
        # Bridge detection (users connecting different communities)
        bridges = self.detect_bridges(communities)
        
        return {
            'communities': communities,
            'influence_scores': influence_scores,
            'betweenness_scores': betweenness_scores,
            'bridge_users': bridges,
            'network_metrics': self.calculate_network_metrics()
        }
    
    def analyze_information_flow(self, content_id: str) -> Dict:
        # Trace how content spreads through the network
        propagation_path = self.trace_content_propagation(content_id)
        
        # Calculate spread velocity
        spread_velocity = self.calculate_spread_velocity(propagation_path)
        
        # Identify amplification points
        amplification_points = self.identify_amplification_points(propagation_path)
        
        return {
            'propagation_path': propagation_path,
            'spread_velocity': spread_velocity,
            'amplification_points': amplification_points,
            'reach_prediction': self.predict_final_reach(propagation_path)
        }

# Advanced Behavioral Pattern Recognition
class AdvancedBehaviorAnalyzer:
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1)
        self.sequence_analyzer = SequenceAnalyzer()
        
    def detect_sophisticated_patterns(self, user_data: Dict) -> Dict:
        patterns = {}
        
        # Temporal pattern analysis
        temporal_patterns = self.analyze_temporal_patterns(user_data['posts'])
        patterns['temporal'] = temporal_patterns
        
        # Language evolution analysis
        language_evolution = self.analyze_language_evolution(user_data['posts'])
        patterns['language_evolution'] = language_evolution
        
        # Cross-platform behavior correlation
        cross_platform = self.analyze_cross_platform_behavior(user_data)
        patterns['cross_platform'] = cross_platform
        
        # Anomaly detection
        anomalies = self.detect_behavioral_anomalies(user_data)
        patterns['anomalies'] = anomalies
        
        return patterns
    
    def analyze_temporal_patterns(self, posts: List[Dict]) -> Dict:
        # Extract posting times
        posting_times = [post['timestamp'] for post in posts]
        
        # Analyze patterns
        hourly_distribution = self.calculate_hourly_distribution(posting_times)
        weekly_pattern = self.calculate_weekly_pattern(posting_times)
        burst_periods = self.detect_burst_periods(posting_times)
        
        # Detect automation indicators
        automation_score = self.calculate_automation_score(posting_times)
        
        return {
            'hourly_distribution': hourly_distribution,
            'weekly_pattern': weekly_pattern,
            'burst_periods': burst_periods,
            'automation_score': automation_score,
            'regularity_index': self.calculate_regularity_index(posting_times)
        }
```

### System Monitoring & Observability

**Comprehensive Monitoring Stack:**

```yaml
# Prometheus Configuration
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "sentinelbert_rules.yml"

scrape_configs:
  - job_name: 'sentinelbert-ingestion'
    static_configs:
      - targets: ['ingestion-service:8080']
    metrics_path: /actuator/prometheus
    
  - job_name: 'sentinelbert-nlp'
    static_configs:
      - targets: ['nlp-service:8000']
    metrics_path: /metrics
    
  - job_name: 'sentinelbert-backend'
    static_configs:
      - targets: ['backend-service:8080']
    metrics_path: /actuator/prometheus

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Custom Alerting Rules
groups:
- name: sentinelbert.rules
  rules:
  - alert: HighIngestionLatency
    expr: histogram_quantile(0.95, rate(ingestion_request_duration_seconds_bucket[5m])) > 2
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High ingestion latency detected"
      description: "95th percentile latency is {{ $value }}s"
      
  - alert: NLPServiceDown
    expr: up{job="sentinelbert-nlp"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "NLP service is down"
      description: "NLP service has been down for more than 1 minute"
      
  - alert: DatabaseConnectionPoolExhausted
    expr: hikaricp_connections_active / hikaricp_connections_max > 0.9
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Database connection pool nearly exhausted"
      description: "Connection pool usage is {{ $value | humanizePercentage }}"
```

**Custom Metrics Collection:**

```java
@Component
public class SentinelBERTMetrics {
    
    private final Counter searchRequestsTotal;
    private final Timer searchRequestDuration;
    private final Gauge activeQueries;
    private final Counter sentimentAnalysisTotal;
    private final Histogram influenceScoreDistribution;
    
    public SentinelBERTMetrics(MeterRegistry meterRegistry) {
        this.searchRequestsTotal = Counter.builder("sentinelbert_search_requests_total")
            .description("Total number of search requests")
            .tag("status", "success")
            .register(meterRegistry);
            
        this.searchRequestDuration = Timer.builder("sentinelbert_search_duration_seconds")
            .description("Search request duration")
            .register(meterRegistry);
            
        this.activeQueries = Gauge.builder("sentinelbert_active_queries")
            .description("Number of currently active queries")
            .register(meterRegistry, this, SentinelBERTMetrics::getActiveQueryCount);
            
        this.sentimentAnalysisTotal = Counter.builder("sentinelbert_sentiment_analysis_total")
            .description("Total sentiment analyses performed")
            .register(meterRegistry);
            
        this.influenceScoreDistribution = Histogram.builder("sentinelbert_influence_score")
            .description("Distribution of influence scores")
            .buckets(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
            .register(meterRegistry);
    }
    
    public void recordSearchRequest(String status, Duration duration) {
        searchRequestsTotal.increment(Tags.of("status", status));
        searchRequestDuration.record(duration);
    }
    
    public void recordSentimentAnalysis(double sentimentScore) {
        sentimentAnalysisTotal.increment();
    }
    
    public void recordInfluenceScore(double score) {
        influenceScoreDistribution.record(score);
    }
    
    private double getActiveQueryCount() {
        return queryService.getActiveQueryCount();
    }
}
```

This comprehensive system design provides a robust, scalable, and secure platform for multi-platform sentiment and behavioral pattern analysis. The architecture leverages modern technologies and best practices to deliver real-time insights while maintaining the highest standards of security and privacy required for law enforcement applications.
